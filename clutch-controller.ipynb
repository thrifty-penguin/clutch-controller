{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development and Simulation of an Adaptive Clutch Modulator\n",
    "\n",
    "Rishon John Moses Paul Sudhagar, Engineering Intern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import h5py\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "from scipy.interpolate import interp1d, CubicSpline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    source_dir      : str   = 'data/amt_raw'\n",
    "    temp_dir        : str   = 'data/amt_temp'\n",
    "    output_dir      : str   = 'artifacts'\n",
    "    scaler_file     : str   = 'scaler.joblib'\n",
    "    model_file      : str   = 'model.joblib'\n",
    "    med_curve_file  : str   = 'median_curves.csv'\n",
    "    idl_curve_file  : str   = 'ideal_curves.csv'\n",
    "    curve_stat_file : str   = 'curve_stats.csv'\n",
    "    n_clusters      : int   = 5\n",
    "    n_trials        : int   = 50\n",
    "    time_step       : float = 0.1\n",
    "    AMTgear_ratios  : dict  = field(default_factory=lambda:{1 : 6.696, 2 : 3.806, 3 : 2.289, 4 : 1.480, 5 : 1.000, 6 : 0.728, -1: 13.862, 0 : 0})\n",
    "    k               : float = 0.107\n",
    "    input_featureset: list  = field(default_factory=lambda:['EngTrq', 'EngSpd', 'tmpCltActTC', 'CurrGr', 'Calc_VehSpd'])\n",
    "    time_col        : str   = 'EventTime'\n",
    "    target_col      : str   = 'ClutchCval'\n",
    "    identifier_col  : str   = 'EngagementEvents'\n",
    "    jerk_weight     : float = 0.6\n",
    "    wear_weight     : float = 0.4\n",
    "    bite_point      : int   = 25\n",
    "    release_point   : int   = 75\n",
    "    engaged         : int   = 0\n",
    "    disengaged      : int   = 100\n",
    "    fallback        : int   = 2\n",
    "    max_clutch_speed: float = 300.0 \n",
    "    plot_style      : list  = field(default_factory=lambda:['science', 'no-latex'])\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 file(s) loaded from data/amt_raw\n"
     ]
    }
   ],
   "source": [
    "def mat_to_csv(mat_name: str, source_dir: str = 'data/amt_raw', temp_dir: str = 'data/amt_temp') -> None:\n",
    "    '''\n",
    "    A function to convert MATLAB .mat files to .csv files.\n",
    "\n",
    "    Parameters:\n",
    "        mat_name (str): The name of the .mat file to be converted.\n",
    "        source_dir (str): The directory where the .mat files are located. Default is 'data/amt_raw'.\n",
    "        temp_dir (str): The directory where the converted .csv files will be saved. Default is 'data/amt_temp'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    testdata={}\n",
    "    mat_path = os.path.join(source_dir, mat_name)\n",
    "\n",
    "    with h5py.File(mat_path, 'r') as f:\n",
    "        for column in f:\n",
    "            if f[column].shape[0] == 1:\n",
    "                fil=np.array(f[column][()])\n",
    "                testdata[f'{column}'] = fil[0]\n",
    "\n",
    "    headers=testdata.keys()\n",
    "\n",
    "    for i in range(0,12):\n",
    "        tim=f't{i}'\n",
    "        param_lst={}\n",
    "        name_lst =[]\n",
    "        if tim in headers:\n",
    "            param_lst['Time']= testdata[tim]\n",
    "            for name in headers:\n",
    "                if name.__contains__(tim) and name != tim and name not in name_lst:\n",
    "                    param_lst[name] = testdata[name]\n",
    "                    param_lst= pd.DataFrame(param_lst)\n",
    "                    test_dir = f'{temp_dir}/{mat_name.split(\".\")[0]}'\n",
    "                    os.makedirs(test_dir, exist_ok=True)\n",
    "                    param_lst.to_csv(os.path.join(test_dir, f'{name}.csv'), index=False)\n",
    "                    name_lst.append(name)\n",
    "\n",
    "\n",
    "\n",
    "source_dir = config.source_dir\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "temp_dir = config.temp_dir\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "mat_files = os.listdir(source_dir)\n",
    "mat_files = [f for f in mat_files if f.endswith('.mat')]\n",
    "\n",
    "if len(mat_files) == 0:\n",
    "    print(f'No valid .mat files found at {source_dir}.')\n",
    "else:\n",
    "    print(f'{len(mat_files)} file(s) loaded from {source_dir}')\n",
    "    for mat_name in mat_files:\n",
    "        mat_to_csv(mat_name, source_dir,temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_align(test_name : str, source_dir : str, time_step: float = 0.01) -> pd.DataFrame:\n",
    "    '''\n",
    "    Aligns all vehicle parameters from a given test to a common time vector using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        test_name (str): Name of the test to be processed.\n",
    "        source_dir (str): Directory containing the test CSV files.\n",
    "        time_step (float): Time step for the common time vector in seconds.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the time-aligned vehicle parameters.\n",
    "    '''\n",
    "    source_dir = f'{source_dir}/{test_name}'\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "    params = os.listdir(source_dir)\n",
    "    params = [f for f in params if f.endswith('.csv')]\n",
    "\n",
    "    df_lst=list()\n",
    "    start = float('inf')\n",
    "    end = float('-inf')\n",
    "\n",
    "    for param in params:\n",
    "        df=pd.read_csv(os.path.join(source_dir, param))\n",
    "        if df.empty:\n",
    "            continue\n",
    "        if 'Time' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        df.sort_values(by='Time', inplace=True)\n",
    "        start = min(start, df['Time'].min())\n",
    "        end = max(end, df['Time'].max())\n",
    "        df_lst.append(df)\n",
    "\n",
    "    if start == float('inf') or end == float('-inf'):\n",
    "        return\n",
    "    \n",
    "    time = np.arange(start, end, time_step)\n",
    "    aligned_df = pd.DataFrame({'Time': time})\n",
    "\n",
    "    for i, df in enumerate(df_lst):\n",
    "        data_col = df.columns[-1]\n",
    "        \n",
    "        INTERP_FUNC = interp1d(df['Time'], df[data_col], kind='linear', \n",
    "                    bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "        if data_col.__contains__('Calculated'):\n",
    "            data_label = data_col.split('_')[1]\n",
    "        elif data_col.__contains__('CAN1'):\n",
    "            data_label = data_col.split('_')[6]\n",
    "        elif data_col.__contains__('SpdR'):\n",
    "            data_label = str(data_col.split('_')[3]+data_col.split('_')[4])\n",
    "        elif data_col.__contains__('Clutch_'):\n",
    "            data_label = str(data_col.split('_')[3]+data_col.split('_')[4])\n",
    "        else:\n",
    "            data_label = data_col.split('_')[3]\n",
    "        \n",
    "        aligned_df[f'{data_label}'] = INTERP_FUNC(time)\n",
    "        aligned_df.dropna(inplace=True)\n",
    "\n",
    "    if aligned_df.empty:\n",
    "        return None\n",
    "    \n",
    "    aligned_df['Time']=aligned_df['Time'].round(decimals=3)\n",
    "\n",
    "    return aligned_df\n",
    "\n",
    "source_dir = config.temp_dir\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "tests = os.listdir(source_dir)\n",
    "\n",
    "time_step = config.time_step # seconds\n",
    "\n",
    "test_data = dict()\n",
    "\n",
    "for test_name in tests:\n",
    "    df = time_align(test_name, source_dir, time_step)\n",
    "    if df is not None:\n",
    "        test_data[f'{test_name.split('.')[0]}_aligned'] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(df: pd.DataFrame, gear_ratios: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "    A function to calculate additional vehicle parameters such as slip power, output shaft speed, and vehicle speed.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the vehicle parameters.\n",
    "        gear_ratios (dict): Dictionary mapping gear numbers to their respective gear ratios.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional calculated parameters.\n",
    "    '''\n",
    "    col=df.columns\n",
    "    if 'InShaftSpd' in col:\n",
    "        df['CurrGr'] = df['CurrGr'].astype(int)\n",
    "        df['Calc_SlipPower'] = (df['EngTrq']) * (df['EngSpd']-df['InShaftSpd']) * (2*np.pi/60)/ 1000\n",
    "        df['Calc_OutShaftSpd'] = df['InShaftSpd'] / df['CurrGr'].map(gear_ratios)\n",
    "        df['Calc_OutShaftSpd'] = df['Calc_OutShaftSpd'].replace([np.nan, np.inf, -np.inf],0)\n",
    "        df['Calc_VehSpd'] = (3/25)*(np.pi)*(config.k)*df['Calc_OutShaftSpd'] # k is a constant that depends on tyre size and final drive ratio\n",
    "    return df\n",
    "\n",
    "tests = test_data.keys()\n",
    "test_data_calc = dict()\n",
    "\n",
    "for test_name in tests:\n",
    "    df = test_data[test_name]\n",
    "    df_calc = calc_params(df,config.AMTgear_ratios)\n",
    "    test_data_calc[f'{test_name}_calc'] = df_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_curves(test_data_calc: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "    A function to identify and extract clutch engagement curves from the test data.\n",
    "    Parameters:\n",
    "        test_data_calc (dict): Dictionary containing the test data DataFrames.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all identified clutch engagement curves with event time.\n",
    "    '''\n",
    "\n",
    "    full_curve_lst = []\n",
    "    offset = 0\n",
    "\n",
    "    for test_name in test_data_calc:\n",
    "            \n",
    "        df = pd.DataFrame.copy(test_data_calc[test_name])\n",
    "\n",
    "        if 'ClutchCval' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if 'ClutchStat' not in df.columns:\n",
    "            curves = df[(df['ClutchCval'] > 10) & (df['ClutchCval'] < 85)].copy()\n",
    "        else:\n",
    "            df['ClutchStat'] = df['ClutchStat'].astype(int)\n",
    "            engaging = df['ClutchStat'] == 1\n",
    "            group = (engaging != engaging.shift()).cumsum()\n",
    "            lengths = engaging.groupby(group).transform('sum')\n",
    "            mask = (df['ClutchStat'] == 1) & (lengths >= 3)\n",
    "            curves = df[mask].copy()\n",
    "        \n",
    "        if curves.empty:\n",
    "            continue\n",
    "\n",
    "        curves = curves.reset_index(drop=True)\n",
    "        time_diff = curves['Time'].diff().fillna(0)\n",
    "        \n",
    "        curves['EngagementEvents'] = (time_diff > 0.015).cumsum() + offset\n",
    "        curves['EventTime'] = curves.groupby('EngagementEvents')['Time'].transform(lambda x: round((x - x.iloc[0]),4))\n",
    "        \n",
    "        full_curve_lst.append(curves)\n",
    "        \n",
    "        offset = curves['EngagementEvents'].max() + 1\n",
    "        \n",
    "    return pd.concat(full_curve_lst, ignore_index=True) if full_curve_lst else pd.DataFrame()\n",
    "\n",
    "df_split = find_curves(test_data_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_splines(df: pd.DataFrame, identifier_col: str, target_col: str, time_col: str, points_per_curve: int = 100) -> dict:\n",
    "    '''\n",
    "    A function to create cubic splines for each clutch engagement event.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the clutch engagement events.\n",
    "        identifier_col (str): Column name identifying each engagement event.\n",
    "        target_col (str): Column name of the target variable for spline fitting.\n",
    "        time_col (str): Column name of the time variable.\n",
    "        points_per_curve (int): Number of points to generate for each spline curve.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are event identifiers and values are arrays of spline points.\n",
    "    '''\n",
    "    splines = {}\n",
    "    events = df.groupby(identifier_col)\n",
    "\n",
    "    for event_num, event in events:\n",
    "        y = event[target_col].values\n",
    "        x = event[time_col].values\n",
    "\n",
    "        if len(x) < 4:\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            spline = CubicSpline(x, y, bc_type='natural')\n",
    "            x_new = np.linspace(0, x.max(), points_per_curve)\n",
    "            y_new = spline(x_new)\n",
    "            splines[event_num] = y_new\n",
    "        except Exception as e:\n",
    "            print(f'Error creating spline for event {event_num}: {e}')\n",
    "            continue\n",
    "    return splines\n",
    "\n",
    "output_dir = config.output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "splines = create_splines(df_split, config.identifier_col, config.target_col, config.time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(df: pd.DataFrame, identifier_col: str, input_featureset: list, time_col: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    A function to aggregate event parameters for each clutch engagement event.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the clutch engagement events.\n",
    "        identifier_col (str): Column name identifying each engagement event.\n",
    "        input_featureset (list): List of feature column names to aggregate.\n",
    "        time_col (str): Column name of the time variable.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing aggregated event parameters.\n",
    "    '''\n",
    "    aggregations = {\n",
    "        feature: ['mean', 'std', ('q25', lambda x: x.quantile(0.25)), ('q75', lambda x: x.quantile(0.75)), 'first']\n",
    "        for feature in input_featureset\n",
    "    }\n",
    "\n",
    "\n",
    "    event_parameters_df = df.groupby(identifier_col).agg(aggregations)\n",
    "    event_parameters_df.columns = ['_'.join(col).strip() for col in event_parameters_df.columns.values]\n",
    "\n",
    "    event_durations = df.groupby(identifier_col)[time_col].max()\n",
    "    event_parameters_df['Duration'] = event_durations\n",
    "\n",
    "    return event_parameters_df\n",
    "\n",
    "event_params = aggregator(df_split, config.identifier_col, config.input_featureset, config.time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_curve_features(y: np.ndarray) -> list:\n",
    "    '''\n",
    "    Extracts statistical and shape features from a spline curve.\n",
    "    Parameters:\n",
    "        y (np.ndarray): Array of spline points.\n",
    "    Returns:\n",
    "        list: A list of extracted features.\n",
    "    '''\n",
    "    return [\n",
    "        np.mean(y), np.median(y), np.std(y), np.min(y), np.max(y),\n",
    "        skew(y), kurtosis(y), y[0], y[-1], y[-1] - y[0],\n",
    "        np.trapezoid(y), len(find_peaks(y)[0])\n",
    "    ]\n",
    "\n",
    "\n",
    "def cluster(splines: dict, max_k: int = 15) -> tuple:\n",
    "    '''\n",
    "    Clusters the spline curves using KMeans and determines the optimal number of clusters using silhouette score.\n",
    "    Parameters:\n",
    "        splines (dict): Dictionary of spline curves.\n",
    "        max_k (int): Maximum number of clusters to test.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the final cluster labels and the fitted scaler.\n",
    "    '''\n",
    "    event_ids = list(splines.keys())\n",
    "\n",
    "    feature_list = [extract_curve_features(vec) for vec in splines.values()]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(feature_list)\n",
    "\n",
    "    best_score = -1.0\n",
    "    optimal_k = 2\n",
    "\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(features_scaled)\n",
    "        score = silhouette_score(features_scaled, kmeans.labels_)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            optimal_k = k\n",
    "\n",
    "    final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10).fit(features_scaled)\n",
    "    final_label_map = dict(zip(event_ids, final_kmeans.labels_))\n",
    "\n",
    "    return final_label_map, scaler\n",
    "\n",
    "scaler_path = os.path.join(config.output_dir, 'scaler.joblib')\n",
    "cluster_labels, scaler = cluster(splines)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f'Scaler saved to {scaler_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(X_train, y_train, X_val, y_val, n_trials=50) -> xgb.XGBClassifier:\n",
    "    '''\n",
    "    Trains and tunes an XGBoost classifier using Optuna for hyperparameter optimisation.\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training feature set.\n",
    "        y_train (pd.Series): Training target labels.\n",
    "        X_val (pd.DataFrame): Validation feature set.\n",
    "        y_val (pd.Series): Validation target labels.\n",
    "        n_trials (int): Number of Optuna trials for hyperparameter optimization.\n",
    "    Returns:\n",
    "        xgb.XGBClassifier: The trained XGBoost classifier with the best hyperparameters.\n",
    "    '''\n",
    "\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    sample_weights = y_train.map(dict(enumerate(weights))).values\n",
    "\n",
    "    def objective(trial) -> float:\n",
    "        '''\n",
    "        An objective function for Optuna to optimize XGBoost hyperparameters.\n",
    "        Parameters:\n",
    "            trial (optuna.trial.Trial): An Optuna trial object.\n",
    "        Returns:\n",
    "            float: The accuracy score on the validation set.\n",
    "        '''\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'random_state': 42,\n",
    "            'num_class': len(np.unique(y_train)),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True)\n",
    "        }\n",
    "\n",
    "\n",
    "        model = xgb.XGBClassifier(**params, use_label_encoder=False, early_stopping_rounds=15)\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        return accuracy_score(y_val, model.predict(X_val))\n",
    "\n",
    "    study = optuna.create_study(direction='maximize', study_name='Clutch Clustering')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(f'Best hyperparameters: {study.best_params}')\n",
    "\n",
    "    final_model = xgb.XGBClassifier(**study.best_params).fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "event_params['cluster_label'] = event_params.index.map(cluster_labels)\n",
    "event_params.dropna(inplace=True)\n",
    "event_params['cluster_label'] = event_params['cluster_label'].astype(int)\n",
    "\n",
    "initial_condition_features = [col for col in event_params.columns if col.endswith('_first')]\n",
    "X = event_params[initial_condition_features]\n",
    "y = event_params['cluster_label']\n",
    "\n",
    "X.columns = [col.replace('_first', '') for col in X.columns]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "final_model = train_tune(X_train, y_train, X_val, y_val, n_trials=config.n_trials)\n",
    "model_save_path = os.path.join(output_dir,config.model_file)\n",
    "joblib.dump(final_model, model_save_path)\n",
    "print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_driveline_jerk(event_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Estimates the total driveline jerk for a single engagement event based on\n",
    "    the rate of change of transmitted torque. This is a more physically\n",
    "    accurate measure of shift quality.\n",
    "    Parameters:\n",
    "        event_df: The full, un-aggregated DataFrame for a single event. \n",
    "                  It must contain 'EngTrq', 'ClutchCval', and 'EventTime'.\n",
    "    Returns:\n",
    "        float: The total integrated jerk for the event.\n",
    "    \"\"\"\n",
    "    event_df = event_df.sort_values(by='EventTime')\n",
    "    transmitted_torque = event_df['EngTrq'].values * (1 - event_df['ClutchCval'].values / 100.0)\n",
    "    torque_derivative = np.gradient(transmitted_torque, event_df['EventTime'].values)\n",
    "    total_jerk = np.sum(np.abs(torque_derivative))\n",
    "    \n",
    "    return total_jerk\n",
    "\n",
    "def estimate_wear_from_event(event_df: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Estimates the total clutch wear for a single engagement event based on\n",
    "    the slip power during the event.\n",
    "    Parameters:\n",
    "        event_df (pd.DataFrame): The full, un-aggregated DataFrame for a single event.\n",
    "    Returns:\n",
    "        float: The total estimated wear energy for the event in Joules.\n",
    "    '''\n",
    "\n",
    "    engine_speed = event_df['EngSpd'] * (np.pi / 30)\n",
    "    input_shaft_speed = event_df['InShaftSpd'] * (np.pi / 30)\n",
    "    slip_speed = np.abs(engine_speed - input_shaft_speed)\n",
    "\n",
    "    power_loss = event_df['EngTrq'] * slip_speed\n",
    "    total_energy = np.trapezoid(power_loss, x=event_df['EventTime'])\n",
    "\n",
    "    return total_energy\n",
    "\n",
    "event_params.dropna(inplace=True)\n",
    "predictions = final_model.predict(event_params)\n",
    "event_params['predicted_cluster'] = predictions\n",
    "\n",
    "costs = []\n",
    "for event_id in event_params.index:\n",
    "    if event_id not in splines: continue\n",
    "    curve = splines[event_id]\n",
    "\n",
    "    event_df = df_split[df_split['EngagementEvents'] == event_id]\n",
    "    wear_cost = estimate_wear_from_event(event_df)\n",
    "    jerk_cost = estimate_driveline_jerk(event_df)\n",
    "    costs.append({'event_id': event_id, 'jerk_cost': jerk_cost, 'wear_cost': wear_cost})\n",
    "cost_df = pd.DataFrame(costs).set_index('event_id')\n",
    "\n",
    "cost_df['jerk_norm'] = (cost_df['jerk_cost'] - cost_df['jerk_cost'].min()) / (cost_df['jerk_cost'].max() - cost_df['jerk_cost'].min())\n",
    "cost_df['wear_norm'] = (cost_df['wear_cost'] - cost_df['wear_cost'].min()) / (cost_df['wear_cost'].max() - cost_df['wear_cost'].min())\n",
    "cost_df['total_cost'] = (config.jerk_weight * cost_df['jerk_norm']) + (config.wear_weight * cost_df['wear_norm'])\n",
    "event_params = event_params.join(cost_df)\n",
    "\n",
    "median_curves = {}\n",
    "ideal_curves = {}\n",
    "cluster_statistics = {}\n",
    "\n",
    "for cluster_id in sorted(event_params['predicted_cluster'].unique()):\n",
    "    cluster_events = event_params[event_params['predicted_cluster'] == cluster_id]\n",
    "    if len(cluster_events) < 1: continue\n",
    "    cluster_vectors = [splines[i] for i in cluster_events.index if i in splines]\n",
    "    if not cluster_vectors: continue\n",
    "    median_curves[cluster_id] = np.median(cluster_vectors, axis=0)\n",
    "\n",
    "    best_event_id = cluster_events['total_cost'].idxmin()\n",
    "    ideal_curves[cluster_id] = splines[best_event_id]\n",
    "\n",
    "\n",
    "    cluster_statistics[cluster_id] = {\n",
    "        'member_ids': list(cluster_events.index),\n",
    "        'mean_jerk': cluster_events['jerk_cost'].mean(),\n",
    "        'mean_wear': cluster_events['wear_cost'].mean(),\n",
    "        'min_cost_event_id': int(best_event_id)\n",
    "    }\n",
    "\n",
    "\n",
    "joblib.dump(median_curves, os.path.join(config.output_dir,config.med_curve_file))\n",
    "joblib.dump(ideal_curves, os.path.join(config.output_dir,config.idl_curve_file))\n",
    "joblib.dump(cluster_statistics, os.path.join(config.output_dir,config.curve_stat_file))\n",
    "\n",
    "plt.style.use(config.plot_style)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "num_clusters = len(median_curves)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_clusters))\n",
    "for i, cluster_id in enumerate(median_curves.keys()):\n",
    "    plt.plot(median_curves[cluster_id], color=colors[i], linestyle='--', label=f'Cluster {cluster_id} Median')\n",
    "    plt.plot(ideal_curves[cluster_id], color=colors[i], label=f'Cluster {cluster_id} Ideal')\n",
    "plt.title('Median (Typical) vs. Ideal (Low-Cost) Engagement Curves per Predicted Profile')\n",
    "plt.xlabel('Normalized Time Points')\n",
    "plt.ylabel('ClutchCval')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real-time controller simulation with rate limiting (accounts for actuator latency)\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'data\\man_raw\\gear_1_manual_params.csv')\n",
    "\n",
    "df.rename(columns={\n",
    "    'Clutch housing Temp': 'tmpCltActTC',\n",
    "    'EngTrq_Cval_PT': 'EngTrq',\n",
    "    'EngSpd_Cval_PT': 'EngSpd',\n",
    "    'VehSpd_Cval_PT': 'Calc_VehSpd'\n",
    "}, inplace=True)\n",
    "\n",
    "if 'CurrGr' not in df.columns:\n",
    "    df['CurrGr'] = 1\n",
    "\n",
    "simulated_clutch_pos = []\n",
    "in_modulation_zone = False\n",
    "predicted_cluster_for_event = None\n",
    "last_clutch_pos = config.engaged\n",
    "last_time = df['Time'].iloc[0]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # --- State & Time Update ---\n",
    "    pedal_pos = row['Clutch_Pedal_Travel']\n",
    "    current_time = row['Time']\n",
    "    dt = current_time - last_time\n",
    "    target_clutch_pos = config.engaged # Default command\n",
    "\n",
    "    # --- Event Detection Logic ---\n",
    "    if config.bite_point < pedal_pos < config.release_point and not in_modulation_zone:\n",
    "        in_modulation_zone = True\n",
    "        \n",
    "        initial_conditions = {\n",
    "            'EngTrq': row['EngTrq'],\n",
    "            'EngSpd': row['EngSpd'],\n",
    "            'tmpCltActTC': row['tmpCltActTC'],\n",
    "            'CurrGr': row['CurrGr'],\n",
    "            'Calc_VehSpd': row['Calc_VehSpd']\n",
    "        }\n",
    "        \n",
    "        feature_df = pd.DataFrame([initial_conditions], columns=final_model.feature_names_in_)\n",
    "        predicted_cluster_for_event = final_model.predict(feature_df)[0]\n",
    "\n",
    "    if pedal_pos <= config.bite_point:\n",
    "        target_clutch_pos = config.engaged\n",
    "        in_modulation_zone = False\n",
    "    elif pedal_pos >= config.release_point:\n",
    "        target_clutch_pos = config.disengaged\n",
    "        in_modulation_zone = False\n",
    "    elif in_modulation_zone and predicted_cluster_for_event is not None:\n",
    "\n",
    "        reference_curve = ideal_curves.get(predicted_cluster_for_event, median_curves.get(config.fallback))\n",
    "        \n",
    "        region_span = config.release_point - config.bite_point\n",
    "        mod_progress = (pedal_pos - config.bite_point) / region_span\n",
    "        \n",
    "        curve_indices = np.linspace(0, 1, len(reference_curve))\n",
    "        target_clutch_pos = np.interp(mod_progress, curve_indices, reference_curve)\n",
    "\n",
    "    max_change = config.max_clutch_speed * dt\n",
    "\n",
    "    if target_clutch_pos > last_clutch_pos:\n",
    "        current_clutch_pos = min(target_clutch_pos, last_clutch_pos + max_change)\n",
    "    else:\n",
    "        current_clutch_pos = max(target_clutch_pos, last_clutch_pos - max_change)\n",
    "\n",
    "    simulated_clutch_pos.append(current_clutch_pos)\n",
    "\n",
    "    last_clutch_pos = current_clutch_pos\n",
    "    last_time = current_time\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df['Time'], simulated_clutch_pos, label='Simulated Controller Output', linewidth=2.5, color='royalblue')\n",
    "plt.plot(df['Time'], df['Clutch_Pedal_Travel'], '--', label='Driver Pedal Input', alpha=0.7, color='gray')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Clutch Position (%)')\n",
    "plt.title('Simulated Clutch Controller with Rate Limiting')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
